# -*- coding: utf-8 -*-
"""Regression_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKKHY91s1t2GgWWwnVw2M7CnMxx_H4zX

## california_housing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.regularizers import l1, l2, l1_l2

# Load the California housing dataset
housing = fetch_california_housing()
X, y = housing.data, housing.target
feature_names = housing.feature_names

print(f"Dataset shape: {X.shape}")
print(f"Features: {feature_names}")
print(f"Target range: [{y.min():.2f}, {y.max():.2f}]")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Build the neural network with regularization and dropout
def create_regression_model(input_dim, l2_reg=0.01, dropout_rate=0.2):
    model = keras.Sequential([
        layers.Dense(64, activation='relu',
                    input_shape=(input_dim,),
                    kernel_regularizer=l2(l2_reg)),
        layers.Dropout(dropout_rate),

        layers.Dense(32, activation='relu',
                    kernel_regularizer=l2(l2_reg)),
        layers.Dropout(dropout_rate),

        layers.Dense(16, activation='relu',
                    kernel_regularizer=l2(l2_reg)),
        layers.Dropout(dropout_rate),

        layers.Dense(1)  # Output layer for regression
    ])

    return model

# Create and compile the model
model = create_regression_model(X_train.shape[1], l2_reg=0.001, dropout_rate=0.3)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# Display model architecture
model.summary()

# Add early stopping
early_stopping = keras.callbacks.EarlyStopping(
    patience=20,
    restore_best_weights=True
)

# Train the model
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=200,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluate the model
test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nTest MSE: {test_loss:.4f}")
print(f"Test MAE: {test_mae:.4f}")

# Make predictions
y_pred = model.predict(X_test_scaled).flatten()

# Calculate additional metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nFinal Metrics:")
print(f"MSE: {mse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ² Score: {r2:.4f}")

# Plot training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.tight_layout()
plt.show()

# Plot predictions vs actual values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()

# Feature importance analysis (using permutation importance)
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error, make_scorer

# Create a scorer for MSE
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Calculate permutation importance
perm_importance = permutation_importance(
    model, X_test_scaled, y_test, n_repeats=10, random_state=42, scoring=mse_scorer
)

# Plot feature importance
plt.figure(figsize=(10, 6))
sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx])
plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
plt.xlabel('Permutation Importance')
plt.title('Feature Importance')
plt.show()



